{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5149 Assignment 2\n",
    "<hr /> \n",
    "\n",
    "# Sentiment Classification - Product reviews\n",
    "<hr /> \n",
    "\n",
    "## Group 30\n",
    "### - Dhanashree Dinkar (29271932)\n",
    "### - Girish Bhatta (29270863)\n",
    "### - Sneha Jambagi (29329493)\n",
    "<hr /> \n",
    "\n",
    "This notebook is used for text pre-processing (for training and test data), feautre generation, selection and model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for feature generation and modeling\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import  svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "from sklearn import svm\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np\n",
    "\n",
    "#Libraries for text pre-processing\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import operator\n",
    "from functools import reduce\n",
    "import json\n",
    "import statistics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading train data\n",
    "df_review = pd.read_csv('train_data.csv')\n",
    "#Reading test data\n",
    "df_review_test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trn_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trn_1</td>\n",
       "      <td>Well this place got me to write my first revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trn_2</td>\n",
       "      <td>A very good Greek restaurant with tasty food. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trn_3</td>\n",
       "      <td>Website says open, Google says open, Yelp says...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trn_4</td>\n",
       "      <td>If I could give zero stars I would. When we wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trn_5</td>\n",
       "      <td>They have great food &amp; definitely excellent se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trn_id                                               text\n",
       "0  trn_1  Well this place got me to write my first revie...\n",
       "1  trn_2  A very good Greek restaurant with tasty food. ...\n",
       "2  trn_3  Website says open, Google says open, Yelp says...\n",
       "3  trn_4  If I could give zero stars I would. When we wa...\n",
       "4  trn_5  They have great food & definitely excellent se..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data to dictionary\n",
    "\n",
    "\n",
    "The data is converted into a dictionary as it increases the speed of processing and improves the code readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data\n",
    "review_dict=df_review.set_index('trn_id').T.to_dict('list')\n",
    "\n",
    "#Test data\n",
    "review_dict_test=df_review_test.set_index('test_id').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data\n",
    "for key,value in review_dict.items():\n",
    "    review_dict[key]=value[0]\n",
    "    \n",
    "#Test data\n",
    "for key,value in review_dict_test.items():\n",
    "    review_dict_test[key]=value[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "The reviews have been tokenized using the below regex which removes punctuations and other special characters and but retains the words with apostrophes. We have considered apostrophes here as they contain some sentiment value like \"can't\", \"don't\", \"won't\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"\\w+(?:[']\\w+)?\")   #Considers  apostrophes, removes special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dicts={}\n",
    "\n",
    "for key,value in review_dict.items():\n",
    "    #Case normalization\n",
    "    lower_normal=value.lower()\n",
    "    #Tokenization using the above regex\n",
    "    tokenized_dicts[key]=tokenizer.tokenize(lower_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dicts_new = tokenized_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dicts_test={}\n",
    "\n",
    "for key,value in review_dict_test.items():\n",
    "    #Case normalization\n",
    "    lower_normal=value.lower()\n",
    "    #Tokenization using the above regex\n",
    "    tokenized_dicts_test[key]=tokenizer.tokenize(lower_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dicts_new_test = tokenized_dicts_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing spaces "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex to remove trailing spaces\n",
    "tokenizer_space = RegexpTokenizer(r\"\\s+\", gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove trailing spaces in the tokens\n",
    "tokenized_dicts_space={}\n",
    "for key,value in tokenized_dicts_new.items():\n",
    "    list1=[]\n",
    "    for each in value:\n",
    "        tokenized_sp = tokenizer_space.tokenize(each)\n",
    "        list1.append(tokenized_sp)\n",
    "    if(len(list1) != 0):\n",
    "        list1=reduce(operator.concat, list1)\n",
    "    tokenized_dicts_space[key]=list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove trailing spaces in the tokens\n",
    "tokenized_dicts_space_test={}\n",
    "for key,value in tokenized_dicts_new_test.items():\n",
    "    list1=[]\n",
    "    for each in value:\n",
    "        tokenized_sp = tokenizer_space.tokenize(each)\n",
    "        list1.append(tokenized_sp)\n",
    "    if(len(list1) != 0):\n",
    "        list1=reduce(operator.concat, list1)\n",
    "    tokenized_dicts_space_test[key]=list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Removing stop words\n",
    "\n",
    "\n",
    "We tried removing the stopwords using different lists which were - \n",
    "\n",
    "- NLTK stop words (25 words in the list) - stopwords.words('english')\n",
    "- NLTK stop words (576 words in the list) - stopwords_en.txt\n",
    "- List containing only 'an', 'the', 'a'.\n",
    "- Customized stop words list which included the following stop words - \n",
    "\n",
    " stopwords_list_new=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'of', 'at', 'by', 'for', 'with',  'about', 'into', 'through', 'during', 'to', 'from', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all','any','both','each','few','more','most','other','some','such','only','own', 'same', 'so', 'too', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',  'ma' ]\n",
    " \n",
    " \n",
    "As it was observed that all the models perform better when no stop words are removed. Therefore, we decided not to remove the stopwords from the corpus. Hence the below code has been commmented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords_list = stopwords.words('english')\n",
    "# stopwords_list\n",
    "\n",
    "#Remove stopwords\n",
    "# tokenized_dicts_stop={}\n",
    "# for key,value in tokenized_dicts_space.items():\n",
    "#     list_stop=[]\n",
    "#     for each in value:\n",
    "#         if each not in stopwords_list:\n",
    "#             list_stop.append(each)\n",
    "#     tokenized_dicts_stop[key]=list_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove words with alphanumeric characters\n",
    "\n",
    "\n",
    "The below set of code was performed in the beginning but since it gave lower accuracy it was commented later on from the final preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non alphanumeric characters from the tokens\n",
    "# tokenized_dicts_alpha={}\n",
    "\n",
    "# for key,value in tokenized_dicts_stop.items():\n",
    "#     list_alpha=[]\n",
    "#     for each in value:\n",
    "#         if each.isalpha():\n",
    "#             list_alpha.append(each)\n",
    "#     tokenized_dicts_alpha[key]=list_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming \n",
    "\n",
    "\n",
    "Stemming was used initially in the pre-processing steps. It was observed that lemmatization gave better results than stemming hence it was not used further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "# tokenized_dicts_stem={}\n",
    "# for key,value in tokenized_dicts_alpha.items():\n",
    "#     list_stem=[]\n",
    "#     for each in value:\n",
    "#         stem='{1}'.format(each, stemmer.stem(each)) \n",
    "#         list_stem.append(stem)\n",
    "#     tokenized_dicts_stem[key]=list_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization \n",
    "\n",
    "\n",
    "Lemmatiztion has been chosen over stemming as it retains the meaning of the word. It produced better results on feature extraction, generation and modeling as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenized_dicts_lemma={}\n",
    "\n",
    "for key,value in tokenized_dicts_space.items():\n",
    "    list_lemma = []\n",
    "    for each in value:\n",
    "        lemma='{1}'.format(each,wordnet_lemmatizer.lemmatize(each)) \n",
    "        list_lemma.append(lemma)\n",
    "    tokenized_dicts_lemma[key]=list_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code was used for further modeling and feature generation by the team-mates.\n",
    "json.dump(tokenized_dicts_lemma, open(\"lemma_notrem_stpwords_fin.txt\",'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dicts_lemma_test={}\n",
    "\n",
    "for key,value in tokenized_dicts_space_test.items():\n",
    "    list_lemma = []\n",
    "    for each in value:\n",
    "        lemma='{1}'.format(each,wordnet_lemmatizer.lemmatize(each)) \n",
    "        list_lemma.append(lemma)\n",
    "    tokenized_dicts_lemma_test[key]=list_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code was used for further modeling and feature generation by the team-mates.\n",
    "json.dump(tokenized_dicts_lemma_test, open(\"test_lemma_nostprem.txt\",'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Pre-processed File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lemma_notrem_stpwords_fin.txt\") as file:\n",
    "    json_data = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data re-structuring and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the dictionary to key value format and converting back to dataframe\n",
    "for k in json_data:\n",
    "    json_data[k] = \" \".join(json_data[k])\n",
    "        \n",
    "\n",
    "tokenised_data =  pd.DataFrame(list(json_data.items()),columns=['id','review'])    \n",
    "tokenised_data.head()\n",
    "tokenised_data.dtypes\n",
    "\n",
    "#reading the labels file\n",
    "\n",
    "train_labels = pd.read_csv(\"train_label.csv\")\n",
    "train_labels.head()\n",
    "train_labels.dtypes\n",
    "\n",
    "#merging the lables with the training data\n",
    "train_data = pd.merge(tokenised_data,train_labels,how = 'inner',left_on = 'id',right_on='trn_id')\n",
    "train_data.head()\n",
    "train_data.drop('trn_id',axis=1,inplace=True)\n",
    "train_data.drop(\"id\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>well this place got me to write my first revie...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a very good greek restaurant with tasty food i...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>website say open google say open yelp say open...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if i could give zero star i would when we walk...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they have great food definitely excellent serv...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  well this place got me to write my first revie...      2\n",
       "1  a very good greek restaurant with tasty food i...      5\n",
       "2  website say open google say open yelp say open...      1\n",
       "3  if i could give zero star i would when we walk...      1\n",
       "4  they have great food definitely excellent serv...      5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sentiment_count=train_data.groupby('label').count()\n",
    "plt.bar(Sentiment_count.index.values, Sentiment_count['review'])\n",
    "plt.xlabel('Review Sentiments')\n",
    "plt.ylabel('Number of Review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation and Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Count Vectorization(Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,2),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(train_data['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Term frequency - Inverse Document Frequency(Tf-Idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(lowercase = True,analyzer = 'word',token_pattern=r'[a-zA-Z0-9]+'\n",
    "                          ,ngram_range = (1,3))\n",
    "text_tf= tf.fit_transform(train_data['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Doc2Vec feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to label each sentences for each observation in the corpus\n",
    "def label_sentences(corpus, label_type):\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into train and test and labelling each of the sentences by learning on the tagged documents using\n",
    "# Tagged Documents\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data.review, train_data.label, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 650000/650000 [00:00<00:00, 3890725.64it/s]\n",
      "100%|██████████| 650000/650000 [00:00<00:00, 3570625.58it/s]\n",
      "100%|██████████| 650000/650000 [00:00<00:00, 3911176.91it/s]\n",
      "100%|██████████| 650000/650000 [00:00<00:00, 3992855.24it/s]\n",
      "100%|██████████| 650000/650000 [00:00<00:00, 3833527.75it/s]\n",
      "100%|██████████| 650000/650000 [00:00<00:00, 3487241.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# instantiating the Doc2Vec constructor specifying the vector-size and initial learning rate of 0.065\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=100, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "\n",
    "# build a vocabulary for all the merged data\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "# train the word vectors using the shuffle function of the tqdm package\n",
    "for epoch in range(5):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the vectors from the trained model from when provided with the trained model instance, corpus size , the size of the \n",
    "# vector and vector type\n",
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "\n",
    "\n",
    "\n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 100, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 100, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_cross_val(model):\n",
    "    # perfroming 10 fold cross validation\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    params = {}\n",
    "    nb = model\n",
    "    gs = GridSearchCV(nb, cv=skf, param_grid=params, return_train_score=False)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the train data for Count Vectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data for count vectoriser\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, train_data['label'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data for Tf-idf Vectoriser\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    text_tf, train_data['label'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fitting Models on Count Vectoriser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Multinomial Navie Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.5443794871794871\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "gs = instantiate_cross_val(model)\n",
    "\n",
    "clf = gs.fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Linear Support Vector Classifier (LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "gs = instantiate_cross_val(model)\n",
    "\n",
    "model=gs.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Linear SVC Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "gs = instantiate_cross_val(model)\n",
    "\n",
    "clf=gs.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting Models of Tf-Idf Vectoriser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "gs = instantiate_cross_val(model)\n",
    "\n",
    "clf = gs.fit(X_train1, y_train1)\n",
    "predicted= clf.predict(X_test1)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test1, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Linear SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "gs = instantiate_cross_val(model)\n",
    "\n",
    "model=gs.fit(X_train1, y_train1)\n",
    "\n",
    "y_pred = model.predict(X_test1)\n",
    "print(\"Linear SVC Accuracy:\",metrics.accuracy_score(y_test1, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "gs = instantiate_cross_val(model)\n",
    "\n",
    "clf=gs.fit(X_train1, y_train1)\n",
    "y_pred = clf.predict(X_test1)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test1, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fitting Models on Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be making use of the vectors returned by the get_vectors function written above to train and build models. We will not be using k-fold cross validation.\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_vectors_dbow, y_train)\n",
    "\n",
    "y_pred = clf.predict(test_vectors_dbow)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making use of the same function to train the documnet vectors on \n",
    "clf = LinearSVC()\n",
    "clf.fit(train_vectors_dbow, y_train)\n",
    "\n",
    "y_pred = clf.predict(test_vectors_dbow)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same document vectors for logistic regression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "\n",
    "y_pred = logreg.predict(test_vectors_dbow)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the findings of the above operations on feature generations and feature extraction methods and of fitting several models on them. \n",
    "<li>The count vectorisation gave decent results on terms of accuracy. However tf-idf feature extraction for unigramsn and tri-grams produces the best results for Linear SVC and Logistic Regression.</li>\n",
    "<li>The Doc2Vec perfroms the worst on the best algorithm of logistic regression and therefore, we do not proceed with fine tuning on models for Doc2Vec feature extraction methodology</li>\n",
    "<li>Since Linear SVC and Logistic Regression provided the best results for tf-idf feature extraction, we perfromed fine tuning only on these models to achieve better results. The comprehensive statistics and metrics of the results of the fine tuning have been recorded in tabular form in the detailed report. In the following sections, we will be touching upon briefly what parameters were fine tuned and what values were found suitable.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimisation and Parameter Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Tuning of  Penalty Parameter C for Linear SVC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter C which penalises a wrong classification thereby, optimising the plane that separates the observations belonging to a particular class and also to maintaing the large for minimum distance between the plane and the observed value. The parameter tuning started with assigning an arbitraty value and then increase and decrease the value of C until we reach the <b>local optima</b>. \n",
    "    \n",
    "We started with values C = 1, 3 and 5. Observed that the accuracy kept decreasing as the value increased from 1 to 3 and to 5. therefore, the value between 1 and 3 were explored and it was finally concluded that the value <b>0.7</b> gives the best accuracy of 0.6523 for trigrams and TF-idf vectorisation on the test dataset. the values were increased and decreased by a factor 0.5 to obtain the approximate direction of increase/decrease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Tuning of Penalty and Solver Parameter for Logistic Regression for Tf-IDF vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar approach was employed as above in the hyperparameter tuning of Logistic Regression as well. We Started with L2 regularisation for Logistic regression with the default solver \"Liblinear\". Although it produced plausibe results, it was highly inefficient as it calcualted coeffients to the quadratic order and the cache management of this solver creates an operational overhead. We therefore, chose 2 solvers for optimisation. i.e \"sag\"(Stochastic average gradient) and \"saga\". These solvers are extremely efficient in terms of managing the memory and overhead operations as well. Also the LibLinear is best suited for problems with \"One vs Rest\" which basically suites a binary classification problem. In this case of multi-class classification problem, we choose multi-class = \"multinomial\" as the parameter. \n",
    "\n",
    "\n",
    "We started with similar values for C for both \"sag\" and \"saga\" solvers. We found that sag performs better compared to saga and we chose to go ahead with sag on this occasion. The following are the findings:\n",
    "\n",
    "<li>The accuracy tends to change in a sinosuidal way. For values within a range it tends to change negatively and positively. With this observation in mind, we started with values 0.5,0.75,2,3,4,5,10. Although the accuracy changed in  wave form, we found that the range which gave the maximum value was between 2 to 3.</li>\n",
    "<li>Upon further tuning we found that value C = 2.55 provides the best result of 0.65800 on \"sag\" solver and \"multinomial\" multiclass. the in-depth observations have been recorded in detail in the final report in a tabular form.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Producing Maximum Accuracy after Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(C=0.7,penalty = 'l2',loss = 'squared-hinge')\n",
    "model.fit(X_train1, y_train1)\n",
    "\n",
    "y_pred = model.predict(X_test1)\n",
    "print(\"Linear SVC Accuracy:\",metrics.accuracy_score(y_test1, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(penalty = 'l2',C = 2.55,solver = 'sag',multi_class = 'multinomial')\n",
    "classifier.fit(X_train1, y_train1)\n",
    "y_pred = classifier.predict(X_test1)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test1, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Predicting for the Entire Train and Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the preprocessed Test File and Dataframe conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_lemma_nostprem.txt\") as file:\n",
    "    json_data = json.loads(file.read())\n",
    "\n",
    "\n",
    "for k in json_data:\n",
    "    json_data[k] = \" \".join(json_data[k])\n",
    "\n",
    "\n",
    "tokenised_test_data =  pd.DataFrame(list(json_data.items()),columns=['id','review'])    \n",
    "tokenised_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectorisation on the test and train dataset on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(lowercase = True,analyzer = 'word',token_pattern=r'[a-zA-Z0-9]+'\n",
    "                          ,ngram_range = (1,3))\n",
    "\n",
    "training_data = tf.fit_transform(train_data['review'])\n",
    "testing_data = tf.transform(tokenised_test_data['review'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Predictions on the best Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "classifier = LogisticRegression(C=2.55,solver='sag',multi_class = 'multinomial')\n",
    "\n",
    "# Training on the entire dataset\n",
    "classifier.fit(training_data,train_data['label'])\n",
    "\n",
    "# predictions on the test data\n",
    "y_pred = classifier.predict(testing_data)\n",
    "\n",
    "# creating a dataframe for the predictions\n",
    "acc = pd.DataFrame(y_pred)\n",
    "\n",
    "# attaching the test_id for all the id's in test data\n",
    "acc[\"test_id\"] = tokenised_test_data[\"id\"]\n",
    "\n",
    "# rearraning of columns \n",
    "acc.columns = [\"label\",\"test_id\"]\n",
    "\n",
    "acc = acc[[\"test_id\",\"label\"]]\n",
    "\n",
    "# writing to a file to check the accuracy in Kaggle\n",
    "acc.to_csv(\"predict_label.csv\",index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
